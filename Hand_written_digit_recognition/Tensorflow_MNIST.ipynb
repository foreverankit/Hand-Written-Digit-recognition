{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network for MNIST Classifaication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is the basic of machine learning also called as the \"Hello World\" of machine learning. The dataset is called MNIST and refers to handwritten digit recognition. The dataset comprises of 28x28 images of handwritten digits(1 per image) and the goal is to write an algorithm that which digit is written. We will build a network of two hidden layers between inputs and outputs.\n",
    "\n",
    "In this project i will be using tensorflow as it is the amazing thing when it comes to machine learning. With hust few lines of code we can do a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist=input_data.read_data_sets(\"MNIST_data/\",one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-6581de42c7fa>:24: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "Epoch 1.Training loss: 0.264. Validation loss: 0.160. Validation accuracy: 94.90%\n",
      "Epoch 2.Training loss: 0.144. Validation loss: 0.152. Validation accuracy: 95.54%\n",
      "Epoch 3.Training loss: 0.123. Validation loss: 0.111. Validation accuracy: 96.84%\n",
      "Epoch 4.Training loss: 0.109. Validation loss: 0.129. Validation accuracy: 96.72%\n",
      "End of training\n"
     ]
    }
   ],
   "source": [
    "# Setting the size of the input, output, hidden layer\n",
    "# Input_size=784 because image dimension is 28x28 which is 784, Output_size=10 because there ar ten digits,\n",
    "# Setting the hidden_layer on our own as it is a hyperparameter.\n",
    "input_size=784\n",
    "output_size=10\n",
    "hidden_layer_size=50\n",
    "\n",
    "# Erasing the old memory\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Setting up placeholders for input and target\n",
    "inputs=tf.placeholder(tf.float32,[None, input_size])\n",
    "targets=tf.placeholder(tf.float32,[None,output_size])\n",
    "\n",
    "# Now setting the weights and biases from the first input layer to the hidden layer with the given dimensions\n",
    "weights_1=tf.get_variable(\"weights_1\",[input_size,hidden_layer_size])\n",
    "biases_1=tf.get_variable(\"biases_1\",[hidden_layer_size])\n",
    "\n",
    "#output folows the linear model with the relu function.\n",
    "outputs_1=tf.nn.relu(tf.matmul(inputs,weights_1)+biases_1)\n",
    "\n",
    "# Setting the weights and biases from the 1st hidden layer to the 2nd hidden layer followed by the output\n",
    "weights_2=tf.get_variable(\"weights_2\",[hidden_layer_size,hidden_layer_size])\n",
    "biases_2=tf.get_variable(\"biases_2\",[hidden_layer_size])\n",
    "\n",
    "outputs_2=tf.nn.relu(tf.matmul(outputs_1,weights_2)+biases_2)\n",
    "\n",
    "#Setting the weights and biases from the 2nt hidden layer to the output without the activation function as we have no hidden layer\n",
    "# left and we have reached the end\n",
    "weights_3=tf.get_variable(\"weights_3\",[hidden_layer_size,output_size])\n",
    "biases_3=tf.get_variable(\"biases_3\",[output_size])\n",
    "\n",
    "outputs=tf.matmul(outputs_2,weights_3)+biases_3\n",
    "\n",
    "#We apply softmax activation and calculate the cross entropy loss\n",
    "#Obtaining very small number jeopardizes our model if we don't employ this function\n",
    "#Logits means unscaled probability. Softmax fits it into a probability distribution\n",
    "loss=tf.nn.softmax_cross_entropy_with_logits(logits=outputs,labels=targets)\n",
    "\n",
    "#Finding the mean loss\n",
    "mean_loss=tf.reduce_mean(loss)\n",
    "\n",
    "#Setting up the optimizer with the learning rate that minimizes the mean loss\n",
    "#Using the Adam Optimizer\n",
    "optimize=tf.train.AdamOptimizer(learning_rate=0.01).minimize(mean_loss)\n",
    "\n",
    "#Predicting accuracy tf.equal returns bool,argmax return the highest indices in the column\n",
    "#Checking whether the predicted output by our model and the target output are same or not ie..1/0\n",
    "out_equals_target=tf.equal(tf.argmax(outputs,1),tf.argmax(targets,1))\n",
    "\n",
    "#Calculating the accuracy\n",
    "accuracy=tf.reduce_mean(tf.cast(out_equals_target,tf.float32))\n",
    "\n",
    "#Session starts\n",
    "sess=tf.InteractiveSession()\n",
    "\n",
    "#Initializing all the variables. Tensorflow takes care of the initialization.\n",
    "initializer=tf.global_variables_initializer()\n",
    "\n",
    "sess.run(initializer)\n",
    "\n",
    "# Setting the batch size to 100.This is a hyperparameter\n",
    "batch_size=100\n",
    "\n",
    "#Batch number will be total_sample/batch_size\n",
    "batches_number=mnist.train._num_examples // batch_size\n",
    "\n",
    "#Setting up th epoch size.This is also a hyperparameter. We can set it to whatever value we can\n",
    "max_epochs=15\n",
    "#Setting the pre_validation_loss to be a large value\n",
    "prev_validation_loss=9999999\n",
    "\n",
    "#Now the iteration starts\n",
    "for epoch_counter in range(max_epochs):\n",
    "    #curr_epoch_loss=0 for every epoch\n",
    "    curr_epoch_loss=0.\n",
    "    #Now we will calculate the epoch loss and the validation loss for each and every batch\n",
    "    #While optimizing we will go front and back and we will keep on optimizing that is reducing the epoch loss\n",
    "    for batch_counter in range(batches_number):\n",
    "        \n",
    "        input_batch, target_batch = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        _,batch_loss = sess.run([optimize,mean_loss],feed_dict={inputs: input_batch,targets: target_batch})\n",
    "        \n",
    "        curr_epoch_loss +=batch_loss\n",
    "        \n",
    "    curr_epoch_loss/=batches_number\n",
    "    \n",
    "    #Now taking the validation data into account we will calculate the validation loss\n",
    "    #Validation loss means going in forward direction\n",
    "    input_batch, target_batch=mnist.validation.next_batch(mnist.validation._num_examples)\n",
    "    \n",
    "    validation_loss, validation_accuracy=sess.run([mean_loss, accuracy],feed_dict={inputs: input_batch, targets: target_batch})\n",
    "    \n",
    "    #Printing the losses and accuracy after each epoch.\n",
    "    print('Epoch '+str(epoch_counter+1)+\n",
    "         '.Training loss: '+'{0:.3f}'.format(curr_epoch_loss)+\n",
    "         '. Validation loss: '+'{0:.3f}'.format(validation_loss)+\n",
    "         '. Validation accuracy: '+'{0:.2f}'.format(validation_accuracy * 100.)+'%')\n",
    "    \n",
    "    #This is very important as we have to keep track whether overfitting is happening or not\n",
    "    #Both the training loss and the validation loss decreases usually but as the validation loss increases it is a signal\n",
    "    #for us that overfitting is going to happen and we should stop training our model.\n",
    "    if validation_loss > prev_validation_loss:\n",
    "        break\n",
    "    prev_validation_loss=validation_loss\n",
    "    \n",
    "print('End of training')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 96.37%\n"
     ]
    }
   ],
   "source": [
    "#Now we will be testing the model with few lines code and printing th accuracy value\n",
    "input_batch, target_batch = mnist.test.next_batch(mnist.test._num_examples)\n",
    "test_accuracy = sess.run([accuracy],feed_dict={inputs: input_batch, targets: target_batch})\n",
    "\n",
    "test_accuracy_percent = test_accuracy[0] * 100.\n",
    "\n",
    "print('Test accuracy: '+'{0:.2f}'.format(test_accuracy_percent)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
